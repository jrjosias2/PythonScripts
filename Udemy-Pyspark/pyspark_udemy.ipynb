{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000022A133035E0>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[3]\").appName(\"UdemyPyspark\").getOrCreate()\n",
    "print(spark)\n",
    "\n",
    "#InferSchema as false, brings everything as String\n",
    "df = (spark.read.format(\"csv\").option(\"header\",\"true\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .option(\"delimiter\",\";\")\n",
    "    .load(\"C:\\\\Users\\\\jrjos\\\\Documents\\\\Python Scripts\\\\PythonScripts\\\\Udemy-Pyspark\\\\Files\\\\covid_mock_file.csv\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- regiao: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- data: string (nullable = true)\n",
      " |-- casosNovos: integer (nullable = true)\n",
      " |-- casosAcumulados: integer (nullable = true)\n",
      " |-- obitosNovos: integer (nullable = true)\n",
      " |-- obitosAcumulados: integer (nullable = true)\n",
      "\n",
      "Dir command da Functions pyspark.sql para coluna regiao\n",
      " ['__add__', '__and__', '__bool__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__invert__', '__iter__', '__le__', '__lt__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rand__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_asc_doc', '_asc_nulls_first_doc', '_asc_nulls_last_doc', '_bitwiseAND_doc', '_bitwiseOR_doc', '_bitwiseXOR_doc', '_contains_doc', '_desc_doc', '_desc_nulls_first_doc', '_desc_nulls_last_doc', '_endswith_doc', '_eqNullSafe_doc', '_ilike_doc', '_isNotNull_doc', '_isNull_doc', '_jc', '_like_doc', '_rlike_doc', '_startswith_doc', 'alias', 'asc', 'asc_nulls_first', 'asc_nulls_last', 'astype', 'between', 'bitwiseAND', 'bitwiseOR', 'bitwiseXOR', 'cast', 'contains', 'desc', 'desc_nulls_first', 'desc_nulls_last', 'dropFields', 'endswith', 'eqNullSafe', 'getField', 'getItem', 'ilike', 'isNotNull', 'isNull', 'isin', 'like', 'name', 'otherwise', 'over', 'rlike', 'startswith', 'substr', 'when', 'withField']\n",
      "\n",
      "Printing Spark DataFrame for 5 first records\n",
      "+------+------+----------+----------+---------------+-----------+----------------+\n",
      "|regiao|estado|      data|casosNovos|casosAcumulados|obitosNovos|obitosAcumulados|\n",
      "+------+------+----------+----------+---------------+-----------+----------------+\n",
      "| Norte|    RO|30/01/2020|         0|              0|          0|               0|\n",
      "| Norte|    RO|31/01/2020|         0|              0|          0|               0|\n",
      "| Norte|    RO|01/02/2020|         0|              0|          0|               0|\n",
      "| Norte|    RO|02/02/2020|         0|              0|          0|               0|\n",
      "| Norte|    RO|03/02/2020|         0|              0|          0|               0|\n",
      "+------+------+----------+----------+---------------+-----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "print(\"Dir command da Functions pyspark.sql para coluna regiao\\n\", dir(F.col(\"regiao\")))\n",
    "print(\"\\nPrinting Spark DataFrame for 5 first records\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seção com exemplos de SELECT e FILTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+-----------+\n",
      "|regiao|estado|casosNovos|obitosNovos|\n",
      "+------+------+----------+-----------+\n",
      "|   Sul|    PR|         0|          0|\n",
      "|   Sul|    PR|         0|          0|\n",
      "|   Sul|    PR|         0|          0|\n",
      "|   Sul|    PR|         0|          0|\n",
      "|   Sul|    PR|         0|          0|\n",
      "|   Sul|    PR|         0|          0|\n",
      "|   Sul|    PR|         0|          0|\n",
      "|   Sul|    PR|         0|          0|\n",
      "|   Sul|    PR|         0|          0|\n",
      "|   Sul|    PR|         0|          0|\n",
      "+------+------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.col(\"regiao\"), \n",
    "            F.col(\"estado\"), \n",
    "            F.col(\"casosNovos\"), \n",
    "            F.col(\"obitosNovos\")).filter(F.col(\"regiao\") == \"Sul\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "| collect_set(estado)|\n",
      "+--------------------+\n",
      "|[AL, PA, CE, RN, ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.collect_set(\"estado\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----------+----------+---------------+-----------+----------------+\n",
      "|  regiao|estado|      data|casosNovos|casosAcumulados|obitosNovos|obitosAcumulados|\n",
      "+--------+------+----------+----------+---------------+-----------+----------------+\n",
      "|   Norte|    AM|15/04/2020|        70|           1554|         16|             106|\n",
      "|   Norte|    AM|16/04/2020|       165|           1719|         18|             124|\n",
      "|   Norte|    AM|17/04/2020|        90|           1809|         21|             145|\n",
      "|   Norte|    AM|18/04/2020|        88|           1897|         16|             161|\n",
      "|   Norte|    AM|19/04/2020|       147|           2044|         21|             182|\n",
      "|   Norte|    AM|20/04/2020|       116|           2160|          3|             185|\n",
      "|   Norte|    AM|21/04/2020|       110|           2270|          8|             193|\n",
      "|   Norte|    AM|22/04/2020|       209|           2479|         14|             207|\n",
      "|   Norte|    AM|23/04/2020|       409|           2888|         27|             234|\n",
      "|   Norte|    AM|24/04/2020|       306|           3194|         21|             255|\n",
      "|   Norte|    AM|25/04/2020|       441|           3635|         32|             287|\n",
      "|Nordeste|    CE|14/04/2020|       205|           2005|         16|             107|\n",
      "|Nordeste|    CE|15/04/2020|       152|           2157|          9|             116|\n",
      "|Nordeste|    CE|16/04/2020|       229|           2386|          8|             124|\n",
      "|Nordeste|    CE|17/04/2020|       298|           2684|         25|             149|\n",
      "|Nordeste|    CE|18/04/2020|       350|           3034|         27|             176|\n",
      "|Nordeste|    CE|19/04/2020|       218|           3252|         10|             186|\n",
      "|Nordeste|    CE|20/04/2020|       230|           3482|         12|             198|\n",
      "|Nordeste|    CE|21/04/2020|       234|           3716|         17|             215|\n",
      "|Nordeste|    CE|22/04/2020|       194|           3910|         18|             233|\n",
      "+--------+------+----------+----------+---------------+-----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"obitosAcumulados>100\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             Estados|             Regioes|\n",
      "+--------------------+--------------------+\n",
      "|[AL, PA, CE, RN, ...|[Nordeste, Sudest...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.collect_set(\"estado\").alias(\"Estados\"), F.collect_set(\"regiao\").alias(\"Regioes\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|collect_set(estado)|\n",
      "+-------------------+\n",
      "|[ES, MG, RJ, SP]   |\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(F.col(\"regiao\") == 'Sudeste').select(F.collect_set(\"estado\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFiltering with & operator\")\n",
    "listFilter = df.filter(F.col(\"regiao\") == 'Centro-Oeste').filter((F.col(\"estado\") == 'DF')\\\n",
    "     | (F.col(\"estado\") == 'MT')).where(\"casosNovos >0\")\n",
    "\n",
    "# Erro que eu não consegui identificar Pesquisar depois o porque salvar diretamente de Pyspark nao rolou\n",
    "#listFilter.write.option(\"header\", \"true\").format(\"csv\").mode(\"append\").save(\"ResultFilter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo para Pandas e então salvando localmente em CSV funcionou\n",
    "# preciso ver qual parametro precisa ser especificado para sobrescrever o arquivo, tive que mudar o nome do arquivo\n",
    "# para poder salvar a segunda operação\n",
    "pdlistFilter = listFilter.toPandas()\n",
    "pdlistFilter.to_csv(r'C:\\Users\\JUNIOJX7\\VsCode\\Udemy-Pyspark\\Files\\output\\datacsv\\pdFilterTest2.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listFilter.select(\"regiao\", \"estado\", \"casosNovos\").sortWithinPartitions(\"estado\", ascending=True).show(5)\n",
    "\n",
    "print(\"Collect_set com os Estados no DF\")\n",
    "listFilter.select(F.collect_set(\"estado\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+----------------+\n",
      "|data      |regiao|estado|obitosAcumulados|\n",
      "+----------+------+------+----------------+\n",
      "|04/04/2020|Norte |AM    |12              |\n",
      "|05/04/2020|Norte |AM    |14              |\n",
      "|06/04/2020|Norte |AM    |19              |\n",
      "|07/04/2020|Norte |AM    |23              |\n",
      "|08/04/2020|Norte |AM    |30              |\n",
      "|09/04/2020|Norte |AM    |40              |\n",
      "|10/04/2020|Norte |AM    |50              |\n",
      "|11/04/2020|Norte |AM    |53              |\n",
      "|12/04/2020|Norte |AM    |62              |\n",
      "|13/04/2020|Norte |AM    |71              |\n",
      "|14/04/2020|Norte |AM    |90              |\n",
      "|15/04/2020|Norte |AM    |106             |\n",
      "|16/04/2020|Norte |AM    |124             |\n",
      "|17/04/2020|Norte |AM    |145             |\n",
      "|18/04/2020|Norte |AM    |161             |\n",
      "|19/04/2020|Norte |AM    |182             |\n",
      "|20/04/2020|Norte |AM    |185             |\n",
      "|21/04/2020|Norte |AM    |193             |\n",
      "|22/04/2020|Norte |AM    |207             |\n",
      "|23/04/2020|Norte |AM    |234             |\n",
      "+----------+------+------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select usando Strings ao invés das Funcoes de Coluna (pyspark.functions) tem o mesmo efeito, porém através das Funcoes, é possível utilizar melhor os recursos\n",
    "df.select(\"data\", \"regiao\", \"estado\", \"obitosAcumulados\").\\\n",
    "    filter(\"regiao = 'Norte'\").filter(\"casosNovos >50 and obitosAcumulados >10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+---------------+-----------+----------------+\n",
      "|regiao|estado|      data|casosNovos|casosAcumulados|obitosNovos|obitosAcumulados|\n",
      "+------+------+----------+----------+---------------+-----------+----------------+\n",
      "| Norte|    RO|30/01/2020|         0|              0|          0|               0|\n",
      "| Norte|    RO|31/01/2020|         0|              0|          0|               0|\n",
      "| Norte|    RO|01/02/2020|         0|              0|          0|               0|\n",
      "| Norte|    RO|02/02/2020|         0|              0|          0|               0|\n",
      "| Norte|    RO|03/02/2020|         0|              0|          0|               0|\n",
      "+------+------+----------+----------+---------------+-----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+------+----------+----------+---------------+-----------+----------------+\n",
      "|regiao|estado|      data|casosNovos|casosAcumulados|obitosNovos|obitosAcumulados|\n",
      "+------+------+----------+----------+---------------+-----------+----------------+\n",
      "| Norte|    RR|30/01/2020|         0|              0|          0|               0|\n",
      "| Norte|    RR|31/01/2020|         0|              0|          0|               0|\n",
      "| Norte|    RR|01/02/2020|         0|              0|          0|               0|\n",
      "| Norte|    RR|02/02/2020|         0|              0|          0|               0|\n",
      "| Norte|    RR|03/02/2020|         0|              0|          0|               0|\n",
      "+------+------+----------+----------+---------------+-----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+------+----------+----------+---------------+-----------+----------------+\n",
      "|  regiao|estado|      data|casosNovos|casosAcumulados|obitosNovos|obitosAcumulados|\n",
      "+--------+------+----------+----------+---------------+-----------+----------------+\n",
      "|Nordeste|    MA|30/01/2020|         0|              0|          0|               0|\n",
      "|Nordeste|    MA|31/01/2020|         0|              0|          0|               0|\n",
      "|Nordeste|    MA|01/02/2020|         0|              0|          0|               0|\n",
      "|Nordeste|    MA|02/02/2020|         0|              0|          0|               0|\n",
      "|Nordeste|    MA|03/02/2020|         0|              0|          0|               0|\n",
      "+--------+------+----------+----------+---------------+-----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filter e Where tem o mesmo efeito, existe a possibilidade de aninhar Filter e Where como no exemplo do bloco de codigo seguinte\n",
    "#Tambem no exemplo 2, é possivel escrever dentro do Filter o comando SQL\n",
    "df.filter(\"regiao = 'Norte' and estado like 'R%'\").show(5)\n",
    "df.filter(\"regiao = 'Norte'\").filter(\"estado not in('AM','AC','RO')\").show(5)\n",
    "\n",
    "listaregiao = [\"Sudeste\", \"Sul\", \"Norte\"]\n",
    "#Uses the \"~\" is the NOT operator for \"isin\" to negate the value\n",
    "df.filter(~F.col(\"regiao\").isin(listaregiao)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seção para exemplos de como usar a clausula WHERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------+---------------+----------------+\n",
      "|      data|      regiao|estado|casosAcumulados|obitosAcumulados|\n",
      "+----------+------------+------+---------------+----------------+\n",
      "|25/04/2020|Centro-Oeste|    GO|            506|              25|\n",
      "|24/04/2020|Centro-Oeste|    GO|            486|              24|\n",
      "|23/04/2020|Centro-Oeste|    GO|            453|              23|\n",
      "|22/04/2020|Centro-Oeste|    GO|            438|              21|\n",
      "|21/04/2020|Centro-Oeste|    GO|            421|              19|\n",
      "+----------+------------+------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.col(\"data\"), \n",
    "    F.col(\"regiao\"),\n",
    "    F.col(\"estado\"), \n",
    "    F.col(\"casosAcumulados\"), \n",
    "    F.col(\"obitosAcumulados\")).where(F.col(\"regiao\") == 'Centro-Oeste').\\\n",
    "        where(F.col(\"estado\") == 'GO').filter(\"casosNovos >0 and obitosAcumulados >0\").orderBy(F.desc(\"casosAcumulados\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seção para WithColum, Cast e \"When\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WithColum adiciona uma nova coluna no DataFrame\n",
    "new_df = df.withColumn(\"soma_casos_novos_acumulados\", F.col(\"casosNovos\")+F.col(\"casosAcumulados\"))\n",
    "new_df.printSchema()\n",
    "new_df.where(\"casosNovos >0\").show(5)\n",
    "\n",
    "cast_df = new_df.select(F.col(\"soma_casos_novos_acumulados\").cast(\"float\"))\n",
    "cast_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excelente exemplo usando de como aplicar WHEN (CASE WHEN) conditionals\n",
    "df_status = df.withColumn(\"Status\", F.when(F.col(\"casosNovos\") >0, F.concat(df.casosNovos, F.lit(\" CasosNovos\"))).\\\n",
    "    otherwise(\"SemCasosNovos\"))\n",
    "df_status.where(df.casosNovos >0).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DMY = df.withColumn(\"Status\", F.when(F.col(\"casosNovos\") >0, F.concat(df.casosNovos, F.lit(\" CasosNovos\"))).\\\n",
    "    otherwise(\"SemCasosNovos\")).\\\n",
    "            withColumn(\"Dia\", F.substring(df.data, 1,2)).\\\n",
    "            withColumn(\"Mes\", F.substring(df.data, 4,2)).\\\n",
    "            withColumn(\"Ano\", F.substring(df.data, 7,4))\n",
    "\n",
    "#df_DMY.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DMY.select(\"data\", \"Dia\", \"Mes\", \"Ano\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DMY.show(10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fazendo cast mantendo no mesmo df\n",
    "df_DMY_casted = df.withColumn(\"Status\", F.when(F.col(\"casosNovos\") >0, F.concat(df.casosNovos, F.lit(\" CasosNovos\"))).\\\n",
    "    otherwise(\"SemCasosNovos\")).\\\n",
    "            withColumn(\"Dia\", F.substring(df.data, 1,2).cast(\"integer\")).\\\n",
    "            withColumn(\"Mes\", F.substring(df.data, 4,2).cast(\"integer\")).\\\n",
    "            withColumn(\"Ano\", F.substring(df.data, 7,4).cast(\"integer\"))\n",
    "\n",
    "df_DMY_casted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DMY_casted.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Struct Type e Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BR_Regiao: string (nullable = true)\n",
      " |-- BR_UF: string (nullable = true)\n",
      " |-- DataRegistro: string (nullable = true)\n",
      " |-- Qtd_casosNovos: integer (nullable = true)\n",
      " |-- Qtd_casosAcumulados: integer (nullable = true)\n",
      " |-- Qtd_obitosNovos: integer (nullable = true)\n",
      " |-- Qtd_obitosAcumulados: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEsse tipo de estrutura é muito valido quando se trabalha com arquivos sem o cabeçalho, ou se deseja alterar/padronizar\\nos nomes existentes nas colunas/cabeçalho de um determinado arquivo/tabela.\\nAtenção pois a quantidade de StructField tem que ser exatamente a mesma de colunas no arquivo, caso contrário nao tera nenhum erro,\\nmas a coluna sera desconsiderada.\\n\\nQuando o header é marcado como False, e nenhum schema e passado como parametro no load, as colunas vem como \\n_c0:\\n_c1:\\n_c...:\\n\\nOutra maneira é usar o metodo toDF após o Load e especificar as colunas \\nex: load(\"C:\\\\Users\\\\JUNIOJX7\\\\Desktop\\\\old\\\\covid_mock_file.csv\").toDF(\"A\", \"B\", \"C\", ...)) \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemaStruct = (StructType([\n",
    "    StructField(\"BR_Regiao\", StringType(), True),\n",
    "    StructField(\"BR_UF\", StringType(), True),\n",
    "    StructField(\"DataRegistro\", StringType(), True),\n",
    "    StructField(\"Qtd_casosNovos\", IntegerType(), True),\n",
    "    StructField(\"Qtd_casosAcumulados\", IntegerType(), True),\n",
    "    StructField(\"Qtd_obitosNovos\", IntegerType(), True),\n",
    "    StructField(\"Qtd_obitosAcumulados\", IntegerType(), True)\n",
    "]))\n",
    "\n",
    "dfStruct = (spark.read.format(\"csv\").option(\"header\",\"true\")\n",
    "    .option(\"inferSchema\",\"false\")\n",
    "    .option(\"delimiter\",\";\")\n",
    "    .load(\"C:\\\\Users\\\\JUNIOJX7\\\\VsCode\\\\Udemy-Pyspark\\\\Files\\\\covid_mock_file.csv\", schema=schemaStruct))\n",
    "\n",
    "dfStruct.printSchema()\n",
    "\n",
    "'''\n",
    "Esse tipo de estrutura é muito valido quando se trabalha com arquivos sem o cabeçalho, ou se deseja alterar/padronizar\n",
    "os nomes existentes nas colunas/cabeçalho de um determinado arquivo/tabela.\n",
    "Atenção pois a quantidade de StructField tem que ser exatamente a mesma de colunas no arquivo, caso contrário nao tera nenhum erro,\n",
    "mas a coluna sera desconsiderada.\n",
    "\n",
    "Quando o header é marcado como False, e nenhum schema e passado como parametro no load, as colunas vem como \n",
    "_c0:\n",
    "_c1:\n",
    "_c...:\n",
    "\n",
    "Outra maneira é usar o metodo toDF após o Load e especificar as colunas \n",
    "ex: load(\"C:\\\\Users\\\\JUNIOJX7\\\\Desktop\\\\old\\\\covid_mock_file.csv\").toDF(\"A\", \"B\", \"C\", ...)) \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------------+--------------+-------------------+---------------+--------------------+\n",
      "|BR_Regiao|BR_UF|DataRegistro|Qtd_casosNovos|Qtd_casosAcumulados|Qtd_obitosNovos|Qtd_obitosAcumulados|\n",
      "+---------+-----+------------+--------------+-------------------+---------------+--------------------+\n",
      "|    Norte|   RO|  19/04/2020|            18|                128|              1|                   4|\n",
      "|    Norte|   RO|  22/04/2020|            24|                223|              1|                   5|\n",
      "|    Norte|   RO|  25/04/2020|            38|                328|              2|                   7|\n",
      "|    Norte|   AC|  13/04/2020|            13|                 90|              1|                   3|\n",
      "|    Norte|   AC|  19/04/2020|            21|                163|              1|                   6|\n",
      "|    Norte|   AC|  20/04/2020|            13|                176|              2|                   8|\n",
      "|    Norte|   AC|  23/04/2020|            13|                227|              2|                  10|\n",
      "|    Norte|   AM|  31/03/2020|            24|                175|              2|                   3|\n",
      "|    Norte|   AM|  03/04/2020|            31|                260|              4|                   7|\n",
      "|    Norte|   AM|  04/04/2020|            51|                311|              5|                  12|\n",
      "|    Norte|   AM|  05/04/2020|           106|                417|              2|                  14|\n",
      "|    Norte|   AM|  06/04/2020|           115|                532|              5|                  19|\n",
      "|    Norte|   AM|  07/04/2020|           104|                636|              4|                  23|\n",
      "|    Norte|   AM|  08/04/2020|           168|                804|              7|                  30|\n",
      "|    Norte|   AM|  09/04/2020|            95|                899|             10|                  40|\n",
      "|    Norte|   AM|  10/04/2020|            82|                981|             10|                  50|\n",
      "|    Norte|   AM|  11/04/2020|            69|               1050|              3|                  53|\n",
      "|    Norte|   AM|  12/04/2020|           156|               1206|              9|                  62|\n",
      "|    Norte|   AM|  13/04/2020|            69|               1275|              9|                  71|\n",
      "|    Norte|   AM|  14/04/2020|           209|               1484|             19|                  90|\n",
      "+---------+-----+------------+--------------+-------------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfStruct.filter(\"Qtd_casosNovos >10\").filter(\"Qtd_obitosNovos >0\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aula06_particionamento no Databricks\n",
    "\n",
    "Consultar o arquivo salvo, diretamente no databricks community para execucao passo a passo, e depois repassar o codigo para execucao direta aqui conectando no DBcks, também fazer a Git push do diretorio de aulas\n",
    "\n",
    "- Pesquisar as funções Partitionby, Repartition and Coalesce, mais exemplos e detalhes sobre RDD tbm\n",
    "\n",
    "*https://sparkbyexamples.com/pyspark/pyspark-partitionby-example/*\n",
    "\n",
    "*https://sparkbyexamples.com/spark/spark-partitioning-understanding/*\n",
    "\n",
    "*https://spark.apache.org/docs/latest/rdd-programming-guide.html*\n",
    "\n",
    "*https://sparkbyexamples.com/pyspark-rdd/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particoes e RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext as SC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(BR_Regiao='Nordeste', BR_UF='MA', DataRegistro='19/02/2020', Qtd_casosNovos=0, Qtd_casosAcumulados=0, Qtd_obitosNovos=0, Qtd_obitosAcumulados=0),\n",
       " Row(BR_Regiao='Nordeste', BR_UF='MA', DataRegistro='20/02/2020', Qtd_casosNovos=0, Qtd_casosAcumulados=0, Qtd_obitosNovos=0, Qtd_obitosAcumulados=0),\n",
       " Row(BR_Regiao='Nordeste', BR_UF='MA', DataRegistro='21/02/2020', Qtd_casosNovos=0, Qtd_casosAcumulados=0, Qtd_obitosNovos=0, Qtd_obitosAcumulados=0),\n",
       " Row(BR_Regiao='Nordeste', BR_UF='MA', DataRegistro='22/02/2020', Qtd_casosNovos=0, Qtd_casosAcumulados=0, Qtd_obitosNovos=0, Qtd_obitosAcumulados=0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dfStruct.rdd.getNumPartitions())\n",
    "dfStructPartition = dfStruct.repartition(F.col(\"BR_Regiao\"))\n",
    "\n",
    "rddStructPartition = dfStructPartition.rdd.repartition(3)\n",
    "print(rddStructPartition.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(BR_Regiao='Nordeste', BR_UF='MA', DataRegistro='19/02/2020', Qtd_casosNovos=0, Qtd_casosAcumulados=0, Qtd_obitosNovos=0, Qtd_obitosAcumulados=0),\n",
       " Row(BR_Regiao='Nordeste', BR_UF='MA', DataRegistro='20/02/2020', Qtd_casosNovos=0, Qtd_casosAcumulados=0, Qtd_obitosNovos=0, Qtd_obitosAcumulados=0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Problema de referencia em configuracoes, que nao sabia aonde alterar, corrigido com copiando o executavel python.exe para python3.exe\n",
    "#Estrutura do collect no RDD, significa [Particao][Linhas] aonde nas linhas é possivel fazer um slice n:n -> Particao 0 | linhas 0 : 2\n",
    "rddStructPartition.glom().collect()[0][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------------+--------------+-------------------+---------------+--------------------+\n",
      "|BR_Regiao|BR_UF|DataRegistro|Qtd_casosNovos|Qtd_casosAcumulados|Qtd_obitosNovos|Qtd_obitosAcumulados|\n",
      "+---------+-----+------------+--------------+-------------------+---------------+--------------------+\n",
      "| Nordeste|   MA|  30/01/2020|             0|                  0|              0|                   0|\n",
      "+---------+-----+------------+--------------+-------------------+---------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfStructPartition.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStructPartition = dfStructPartition.withColumn(\"Dia\", F.substring(dfStructPartition.DataRegistro, 1,2)).\\\n",
    "            withColumn(\"Mes\", F.substring(dfStructPartition.DataRegistro, 4,2)).withColumn(\"Ano\", F.substring(dfStructPartition.DataRegistro, 7,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStructPartition.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfStructPartition' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23412/812170727.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# sendo possível utilizar o SLICE para colunas [N][n:n]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Linha 0 Row(BR_Regiao='Nordeste', BR_UF='MA', DataRegistro='19/02/2020', Qtd_casosNovos=0, Qtd_casosAcumulados=0, Qtd_obitosNovos=0, Qtd_obitosAcumulados=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdfStructPartition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dfStructPartition' is not defined"
     ]
    }
   ],
   "source": [
    "# Usado para trazer todas as linhas do dataFrame por é possível utilizar estrutura de Matriz para recuperar os dados\n",
    "# dfStructPartition.collect()[0] retornando o indice da linha e todas as colunas\n",
    "# dfStructPartition.collect()[0][0] retornando o indice linha e coluna do conjunto \n",
    "# sendo possível utilizar o SLICE para colunas [N][n:n]\n",
    "# Linha 0 Row(BR_Regiao='Nordeste', BR_UF='MA', DataRegistro='19/02/2020', Qtd_casosNovos=0, Qtd_casosAcumulados=0, Qtd_obitosNovos=0, Qtd_obitosAcumulados=0)\n",
    "dfStructPartition.collect()[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erro nao funciona para salvar localmente, mas funcionou para salvar no DataBricks Analisar depois, \n",
    "# pois mesmo direcionando o local para o mesmo caminho de load deu erro, mesmo com os parametros em options\n",
    "# 16-JUN Deve ser algum erro de dependências entre as bibliotecas, como foi com o python3.exe, pois ao rodar o comando olhando para a file explorer ao lado, os arquivos começam a ser salvos\n",
    "# porém acontece um erro de linkNative e a operação e encerrada, Estudar melhor: https://rollbar.com/blog/java-unsatisfiedlinkerror-runtime-error/#:~:text=lang.,a%20subclass%20of%20the%20java.\n",
    "\n",
    "dfStructPartition.write.option(\"header\", True).csv(\"C:/Users/JUNIOJX7/VsCode/Udemy-Pyspark/Files/output/datacsv2/FileOutput.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste para manual Struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deptDF.write.csv(\"C:/Users/JUNIOJX7/VsCode/Udemy-Pyspark/Files/output/datacsv2/FileOutput.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 07 - Agrupamentos\n",
    "\n",
    "Consultar codigo no Databricks community, pois o notebook estara com os comentarios lá, porém tentar aplicar essas funções com o conjunto de dados carregados localmente aqui também"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStructPartition.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUM \n",
    "dfStructPartition.groupBy(\"BR_Regiao\", \"BR_UF\").sum(\"Qtd_obitosAcumulados\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count\n",
    "dfStructPartition.groupBy(\"BR_Regiao\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min\n",
    "dfStructPartition.groupBy(\"BR_Regiao\").min(\"Qtd_obitosAcumulados\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX Simples\n",
    "#dfStructPartition.groupBy(\"BR_Regiao\").max(\"Qtd_obitosNovos\").show()\n",
    "\n",
    "#MAX com funcao de ordenacao sort()\n",
    "# ERRO abaixo, procurar entender o porque usando ou não o agg e a funcao sort esta dando erro aqui\n",
    "# O erro acontecia porque a descricao da coluna incorpora o nome da funcao, ficava max(Qtd_obitosNovos), o que gerava erro ao apontar somente para nome da coluna\n",
    "# nesse caso deve-se colocar um alias, e utilizar a funcao agg, senao retorna erro.\n",
    "#dfStructPartition.groupBy(\"BR_Regiao\").agg(F.max(\"Qtd_obitosNovos\")).sort(F.desc(\"Qtd_obitosNovos\")).show()\n",
    "\n",
    "dfStructPartition.groupBy(\"BR_Regiao\").agg(F.max(\"Qtd_obitosNovos\").alias(\"VolumeObitosTotal\")).sort(F.desc(\"VolumeObitosTotal\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AVG e MEAN --> Existe uma diferença na aplicação entre os métodos, nas aulas de DataScience com Python do CloudAcademy e no notebook relacionado tem mais detalhes entre as diferenças\n",
    "\n",
    "dfStructPartition.groupBy(\"BR_Regiao\").agg(F.mean(\"Qtd_obitosNovos\").alias(\"MeanObitosNovos\")).sort(F.desc(\"MeanObitosNovos\")).show()\n",
    "dfStructPartition.groupBy(\"BR_Regiao\").agg(F.avg(\"Qtd_obitosNovos\").alias(\"AvgObitosNovos\")).sort(F.desc(\"AvgObitosNovos\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupando, aplicado funcoes de agregacao, e filtrando o resultado.\n",
    "dfStructPartition.groupBy(\"BR_Regiao\", \"BR_UF\").sum(\"Qtd_casosAcumulados\", \"Qtd_obitosAcumulados\")\\\n",
    "    .filter(F.col(\"sum(Qtd_casosAcumulados)\") > 5000).sort(F.desc(\"sum(Qtd_casosAcumulados)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exemplo animal de agrupamento, por agregacao, utilizando outras funcoes de SQL atraves da Functions, e criando coluna para atribuir resultado no DF\n",
    "\n",
    "df2_StructPartition = dfStructPartition.groupBy(\"BR_Regiao\", \"BR_UF\").agg(F.sum(\"Qtd_casosNovos\").alias(\"VolumeCasosNovos\"),\\\n",
    "     F.sum(\"Qtd_obitosNovos\").alias(\"VolumeObitosNovos\"), F.format_number(F.avg(\"Qtd_casosNovos\"), 2).alias(\"Avg CasosNovos\"),\\\n",
    "     F.max(\"Qtd_obitosAcumulados\").alias(\"Max_ObitosAcumulados\")).\\\n",
    "         withColumn(\"pct_Casos\", F.format_number((F.col(\"VolumeObitosNovos\") / F.col(\"VolumeCasosNovos\") * 100), 2).cast(\"float\"))\\\n",
    "    .filter(F.col(\"VolumeCasosNovos\") > 1000).sort(F.desc(\"VolumeCasosNovos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_StructPartition.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 08 Funcoes de Agregacoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retornar a qtd de valores distintos de um dataframe\n",
    "\n",
    "aproxFuncCount = dfStructPartition.select(F.approx_count_distinct(\"BR_UF\")).collect()[0][0]\n",
    "print(f\"approx_count_distinct:  {aproxFuncCount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retorna os valores do dataframe em uma lista, independente de valores repetidos ou não\n",
    "dfStructPartition.select(F.collect_list(\"BR_UF\")).show(truncate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retorna o valor do dataframe em uma lista, somente os valores unicos, nao retorna os duplicados\n",
    "dfStructPartition.select(F.collect_set(\"BR_UF\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retorna o valor do dataframe em uma lista, nao retorna os valores unicos\n",
    "dfStructPartition.select(F.collect_set(\"BR_Regiao\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retorna a qtd de itens unicos no dataframe\n",
    "dfStructPartition.select(F.countDistinct(\"BR_UF\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retorna o valor do dataframe em uma lista, nao retorna os valores unicos\n",
    "dfStructPartition.select(F.first(\"BR_UF\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 09 Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montagem dos DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataEmp = [\n",
    "    (1, \"Anderson\", 10000),\n",
    "    (2, \"Kenedy\", 20000),\n",
    "    (3, \"Billy\", 23000),\n",
    "    (4, \"Andy\", 23000),\n",
    "    (5, \"Mary\", 24000),\n",
    "    (6, \"Eduardo\", 19000),\n",
    "    (7, \"Mendes\", 15000),\n",
    "    (8, \"Keyth\", 18000),\n",
    "    (9, \"Truman\", 21000),\n",
    "]\n",
    "schemaEmp = [\"id\", \"name\", \"salary\"]\n",
    "\n",
    "dfEmp = spark.createDataFrame(data=dataEmp, schema=schemaEmp)\n",
    "dfEmp.printSchema()\n",
    "dfEmp.show(5)\n",
    "\n",
    "dataPlace = [\n",
    "    (1, \"Delhi\", \"India\"),\n",
    "    (2, \"Tamil nadu\", \"India\"),\n",
    "    (3, \"London\", \"UK\"),\n",
    "    (4, \"Sydney\", \"Australia\"),\n",
    "    (8, \"New York\", \"USA\"),\n",
    "    (9, \"California\", \"USA\"),\n",
    "    (10, \"New Jersey\", \"USA\"),\n",
    "    (11, \"Texas\", \"USA\"),\n",
    "    (12, \"Chicago\", \"USA\"),\n",
    "]\n",
    "schemaPlace = [\"id\", \"place\", \"country\"]\n",
    "\n",
    "dfPlace = spark.createDataFrame(data=dataPlace, schema=schemaPlace)\n",
    "dfPlace.printSchema()\n",
    "dfPlace.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code's On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basta usar o join, informar qual o dataframe, no on['coluna'], how='' é o tipo inner, outer, right ou left\n",
    "# Pesquisar mais, porque as explicações da aula foram toscas\n",
    "dfEmp.join(dfPlace, on=['id'], how='outer').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEmp.join(dfPlace, on=['id'], how='left').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEmp.join(dfPlace, on=['id'], how='right').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEmp.join(dfPlace, on=['id'], how='inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left_anti pesquisar, mas basicamente traz todas as linhas do Dataframe a esquerda que nao tem na direita\n",
    "# tem o anti que parece exercer a mesma funcao, pesquisar e anotar as diferenças depois\n",
    "dfEmp.join(dfPlace, on=['id'], how='left_anti').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pesquisar mas basicamente traz todos os valores a esquerda que existem dados correspondentes a direita.\n",
    "dfEmp.join(dfPlace, on=['id'], how='left_semi').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traz todos os registros\n",
    "dfEmp.join(dfPlace, on=['id'], how='full').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 10 Funcoes de Window e Particoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window as W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montagem do DataFrame\n",
    "Como no exemplo com o dataFrame de covid, tem muitos registros para acompanhar, vou criar o DataFrame proposto no curso com exemplo de um Dept de colaboradores e localidade para ver melhor os efeitos de cada uma das funcoes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Anderson\",\"Sales\",\"NY\",90000),\n",
    "    (\"Kenedy\",\"Sales\",\"CA\",86000),\n",
    "    (\"Kenny\",\"Sales\",\"CA\",86000),\n",
    "    (\"Billy\",\"Sales\",\"NY\",81000),\n",
    "    (\"Andy\",\"Finance\",\"CA\",90000),\n",
    "    (\"Mary\",\"Finance\",\"NY\",99000),\n",
    "    (\"Eduardo\",\"Finance\",\"NY\",83000),\n",
    "    (\"Mendes\",\"Finance\",\"CA\",79000),\n",
    "    (\"Keyth\",\"Marketing\",\"CA\",80000),\n",
    "    (\"Truman\",\"Marketing\",\"NY\",91000),\n",
    "    (\"Josa\", \"Finance\", \"AL\",55000)\n",
    "  ]\n",
    "schema = [\"name\",\"dep_name\",\"state\",\"salary\"]\n",
    "\n",
    "dfAula10 = spark.createDataFrame(data=data, schema = schema)\n",
    "dfAula10.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAula10.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code's On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria resultado da funcao de Window pela particao do Departamento, ordenada pelo Salario decrescente dos colaboradores\n",
    "wt01 = W.partitionBy(F.col(\"dep_name\")).orderBy(F.desc(\"salary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RowNumber, lista o numero das linhas por Departamento do maior salario para o menor\n",
    "dfAula10.withColumn(\"rowNumber\", F.row_number().over(wt01)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiz o teste colocando uma nova Window WindowTest01_1 para ver o resultado de ter o departamento e o estado particionados\n",
    "# E nesse caso, o resultado mostra o meu salario primeiro, mesmo sendo o menor de todos, pois a particao estado AL tem prioridade ordem alfabetica\n",
    "# que os demais, no exemplo acima, como o Estado não é uma partição, o meu nome aparece em último entre os departamentos.\n",
    "wt01_1 = W.partitionBy(F.col(\"dep_name\"), F.col(\"state\")).orderBy(F.desc(\"salary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAula10.withColumn(\"rowNumber\", F.row_number().over(wt01_1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank mostra a posicao dos valores Pelo Particionamento, ordenado pelo Salario, considerando ordem alfabetica para os demais itens\n",
    "# por isso Kenedy esta na rank 2 acima do Kenny também com rank 2 com base no Dept e Salario, e deixa a lacuna para o rank 3 \n",
    "# (pertencente a Kenny segundo em desempate no rank 2) colocando Billy com o Rank 4\n",
    "# Resumindo a Rank coloca em Rank os itens estipulados pelo Particionamento e Orderby, agrupando todos os valores empatados, porem guardando a posicao 1,2,[2,2,2],6,7\n",
    "# Por isso em Sales vemos 1,2,2,4 ele mantém a posição do elemento com Rank\n",
    "dfAula10.withColumn(\"Rank\", F.rank().over(wt01)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense_rank diferente do Rank, preenche a posição seguinte a que teve agrupamento por empate do Rank anterior, por isso aqui Billy esta com Rank 3, ao invés de 4\n",
    "dfAula10.withColumn(\"Dense_Rank\", F.dense_rank().over(wt01)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pesquisar, explicacao tosca\n",
    "dfAula10.withColumn(\"Percent_Rank\", F.percent_rank().over(wt01)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ntile(nSlices) por nSlices ele agrupa os valores do particionamento nessa qtd atribuido na função\n",
    "# agrupando um subconjunto de dados ordenados\n",
    "# com isso poderiamos dizer que o Grupo 1 contem o maiores salarios por departamento, grupo 2 os segundos, e grupo 3 os que ganham menos\n",
    "# Se tivessemos mais pessoas ele agruparia 3 pessoas por Grupo ->> TO-DO colocar mais pessoas por departamento e rodar novamente pra ver a efetividade\n",
    "dfAula10.withColumn(\"Ntile\", F.ntile(3).over(wt01)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Pesquisar mais**, pega a distribuicao culmulativa da particao\n",
    "dfAula10.withColumn(\"cume_dist\", F.cume_dist().over(wt01)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicacao Tosca, mas funcao pode ser importante, pesquisar mais a respeito pode ser valido PRA CARALHO pra DataScience\n",
    "# Mas basicamente, escolhe qual a coluna que quer atribuir o Lag, e a Qtd de Lag, que signfica voltar essa Qtd na Coluna dentro do DataFrame\n",
    "# Por isso Eduardo tem o Lag de 99000, Josa de 83000\n",
    "dfAula10.withColumn(\"Lag\", F.lag(\"salary\", 2).over(wt01)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar ao Lag, porém ao invés de percorrer para Tras, vai para Frente no Conjunto.\n",
    "dfAula10.withColumn(\"Lead\", F.lead(\"salary\", 2).over(wt01)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datatypes Complexos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, DoubleType, IntegerType, LongType, StringType, StructType, StructField, BooleanType, MapType\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Trabalhando com Arrays, no import acima, trouxe alguns tipos do sql Types, o legal aqui foi trabalhar com os Arrays como pontos\n",
    "No Bloco abaixo, a definição do schema é interessante mostrar que o campo Points e o user_state tem tipos diferentes que eu havia visto ate aqui\n",
    "O legal é como os valores são mostrados em operacoes no DF, Ver a execucao do bloco 3 df.show()\n",
    "'''\n",
    "data = [Row(\"Kenny\",\"\",10,[50,90,80],{\"status\":\"Active\"}), \n",
    "        Row(\"Elis\",\"Robert\",20,[10,56,43,20],{\"status\":\"Inactive\"}), \n",
    "        Row(\"Myck\",\"Mendes\",30,[18,50,32],{\"status\":\"Active\"}), \n",
    "        Row(\"Edson\",\"Eliot\",40,[60,87,3],{\"status\":\"Active\"}) \n",
    "      ]\n",
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme = StructType([\n",
    "         StructField('firstname', StringType(), True),\n",
    "         StructField('middlename', StringType(), True),\n",
    "         StructField('age', IntegerType(), True),\n",
    "         StructField(\"points\", ArrayType(StringType()), True),\n",
    "         StructField(\"user_state\", MapType(StringType(),StringType()), True)        \n",
    "         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[firstname: string, middlename: string, age: int, points: array<string>, user_state: map<string,string>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = rdd.toDF(schema=scheme)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---+----------------+--------------------+\n",
      "|firstname|middlename|age|          points|          user_state|\n",
      "+---------+----------+---+----------------+--------------------+\n",
      "|    Kenny|          | 10|    [50, 90, 80]|  {status -> Active}|\n",
      "|     Elis|    Robert| 20|[10, 56, 43, 20]|{status -> Inactive}|\n",
      "|     Myck|    Mendes| 30|    [18, 50, 32]|  {status -> Active}|\n",
      "|    Edson|     Eliot| 40|     [60, 87, 3]|  {status -> Active}|\n",
      "+---------+----------+---+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Da pra ver que os pontos são mostrados exatamente como uma Lista (array), e o ponto de Map, é atribuido como chave:valor\n",
    "'''\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[firstname: string, point: string, status: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Apois usar a funcao Explode, os pontos sao atribuidos a cada linha no DF\n",
    "df02 = (df.withColumn(\"point\", F.explode(\"points\"))\n",
    "  .select(\"firstname\",\"point\",\"user_state.status\")        \n",
    ")\n",
    "\n",
    "display(df02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------+\n",
      "|firstname|point|  status|\n",
      "+---------+-----+--------+\n",
      "|    Kenny|   50|  Active|\n",
      "|    Kenny|   90|  Active|\n",
      "|    Kenny|   80|  Active|\n",
      "|     Elis|   10|Inactive|\n",
      "|     Elis|   56|Inactive|\n",
      "|     Elis|   43|Inactive|\n",
      "|     Elis|   20|Inactive|\n",
      "|     Myck|   18|  Active|\n",
      "|     Myck|   50|  Active|\n",
      "|     Myck|   32|  Active|\n",
      "|    Edson|   60|  Active|\n",
      "|    Edson|   87|  Active|\n",
      "|    Edson|    3|  Active|\n",
      "+---------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df02.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aqui o filter, usa a funcao array_contains, para filtrar a coluna do tipo array, e qual o valor que deseja buscar, e cria (withColum) a coluna get_point, e busca somente o elemento 2 da coluna points.\n",
    "#isso com o uso da funcao element_at(Coluna, index), resultado no bloco abaixo, pode ser comparado com os pontos do resultado do bloco acima.\n",
    "df03 = df.filter(F.array_contains(F.col(\"points\"), \"50\")).withColumn(\"get_point\", F.element_at(F.col(\"points\"), 2))\n",
    "display(df03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df03.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df04 = df.withColumn(\"get_point\", F.element_at(F.col(\"points\"), 1))\n",
    "display(df04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df04.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agrupamento agreegado do DF acima, pelo Status dos usuários.\n",
    "df05 = df04.groupBy(\"firstname\", \"user_state.status\").agg(F.collect_set(\"get_point\").alias(\"Points\"))\n",
    "\n",
    "display(df05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df05.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df05 = df05.withColumnRenamed(\"Points\",\"New_Points\")\n",
    "df05.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Defined Functions, Json, Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "foi pessima, buscar mais referencias por fora porque no curso foi ridiculamente péssimo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "#Cria uma VIEW temporaria, simples, segue a estrututa, especifica o tipo de arquivo e o local, e por boa pratica, incluir o filename, pois eh possivel, ler uma lista de arquivos.\n",
    "\n",
    "(spark.read.option(\"header\",\"true\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .option(\"delimiter\",\";\")\n",
    "    .csv(\"C:\\\\Users\\\\JUNIOJX7\\\\VsCode\\\\Udemy-Pyspark\\\\Files\\\\covid_mock_file.csv\")\n",
    "    .withColumn(\"fileName\", input_file_name())).createOrReplaceTempView(\"vw_tbl_covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+---------------+-----------+----------------+--------------------+\n",
      "|regiao|estado|      data|casosNovos|casosAcumulados|obitosNovos|obitosAcumulados|            fileName|\n",
      "+------+------+----------+----------+---------------+-----------+----------------+--------------------+\n",
      "| Norte|    AM|25/03/2020|         7|             54|          1|               1|file:///C:/Users/...|\n",
      "| Norte|    AM|26/03/2020|        13|             67|          0|               1|file:///C:/Users/...|\n",
      "| Norte|    AM|27/03/2020|        14|             81|          0|               1|file:///C:/Users/...|\n",
      "| Norte|    AM|28/03/2020|        30|            111|          0|               1|file:///C:/Users/...|\n",
      "| Norte|    AM|29/03/2020|        29|            140|          0|               1|file:///C:/Users/...|\n",
      "| Norte|    AM|30/03/2020|        11|            151|          0|               1|file:///C:/Users/...|\n",
      "| Norte|    AM|01/04/2020|        25|            200|          0|               3|file:///C:/Users/...|\n",
      "| Norte|    AM|02/04/2020|        29|            229|          0|               3|file:///C:/Users/...|\n",
      "| Norte|    AM|03/04/2020|        31|            260|          4|               7|file:///C:/Users/...|\n",
      "| Norte|    AM|04/04/2020|        51|            311|          5|              12|file:///C:/Users/...|\n",
      "+------+------+----------+----------+---------------+-----------+----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Em seguida é possivel executar qualquer comando SQL, utilizando a funcao sql.\n",
    "spark.sql(\"SELECT * FROM vw_tbl_covid WHERE regiao='Norte' AND (estado='AM' OR estado='RR') AND obitosAcumulados >0 AND data BETWEEN '01/03/2020' AND '30/03/2020';\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|      regiao|Estados|\n",
      "+------------+-------+\n",
      "|    Nordeste|      9|\n",
      "|         Sul|      3|\n",
      "|     Sudeste|      4|\n",
      "|Centro-Oeste|      4|\n",
      "|       Norte|      7|\n",
      "+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT regiao, COUNT(DISTINCT(estado)) as Estados FROM vw_tbl_covid GROUP BY regiao HAVING Estados >2;\").show()\n",
    "#spark.sql(\"SELECT * FROM vw_tbl_covid;\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|estado|regiao|\n",
      "+------+------+\n",
      "|    PA| Norte|\n",
      "|    RR| Norte|\n",
      "|    AC| Norte|\n",
      "|    AP| Norte|\n",
      "|    RO| Norte|\n",
      "|    AM| Norte|\n",
      "|    TO| Norte|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT DISTINCT estado, regiao FROM vw_tbl_covid WHERE regiao = 'Norte'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT SUM(qtd_estados) as Total_UF FROM (SELECT regiao, COUNT(DISTINCT(estado)) as qtd_estados FROM vw_tbl_covid GROUP BY 1);\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(\"SELECT regiao, SUM(UFs) AS TOTAL_UF FROM (SELECT regiao, COUNT(DISTINCT(estado)) as UFs FROM vw_tbl_covid GROUP BY regiao) GROUP BY regiao ORDER BY regiao;\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conceito captado, interessante eh que dinamicamente em tempo de execucao e possivel estabelecer criterios e salvar datasets, views e mesmo tabelas\n",
    " spark.sql(\"SELECT * FROM vw_tbl_covid WHERE data <='30/01/2020' AND regiao='Sul' AND obitosAcumulados <>0\\\n",
    "    UNION ALL SELECT * FROM vw_tbl_covid WHERE data <='30/01/2020' AND regiao='Sudeste' AND obitosAcumulados <>0;\").createOrReplaceTempView(\"vw_union_sulsudeste\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT DISTINCT data FROM vw_tbl_covid WHERE casosNovos <>0;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT DISTINCT SUBSTRING(data, 4,2) FROM vw_tbl_covid;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCOES DE JANELA NO SQL\n",
    "#CRIANDO O DF\n",
    "\n",
    "data = [(\"Anderson\",\"Sales\",\"NY\",90000),\n",
    "    (\"Kenedy\",\"Sales\",\"CA\",86000),\n",
    "    (\"Kenny\",\"Sales\",\"CA\",86000),\n",
    "    (\"Billy\",\"Sales\",\"NY\",81000),\n",
    "    (\"Andy\",\"Finance\",\"CA\",90000),\n",
    "    (\"Mary\",\"Finance\",\"NY\",99000),\n",
    "    (\"Eduardo\",\"Finance\",\"NY\",83000),\n",
    "    (\"Mendes\",\"Finance\",\"CA\",79000),\n",
    "    (\"Keyth\",\"Marketing\",\"CA\",80000),\n",
    "    (\"Truman\",\"Marketing\",\"NY\",91000),\n",
    "    (\"Josa\", \"Finance\", \"AL\",55000)\n",
    "  ]\n",
    "schema = [\"name\",\"dep_name\",\"state\",\"salary\"]\n",
    "\n",
    "spark.createDataFrame(data=data, schema=schema).createOrReplaceTempView(\"vw_tbl_emp_window_f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usando o Rank\n",
    "spark.sql(\"SELECT name, dep_name, RANK() OVER(PARTITION BY dep_name ORDER BY salary DESC) AS rank FROM vw_tbl_emp_window_f;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usando Dense_rank\n",
    "spark.sql(\"SELECT name, dep_name, state, DENSE_RANK() OVER(PARTITION BY dep_name ORDER BY salary DESC) AS rank FROM vw_tbl_emp_window_f;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usando Dense_rank com os parametros ROWS BETWEEN....\n",
    "spark.sql(\"SELECT name, dep_name, DENSE_RANK() OVER(PARTITION BY dep_name ORDER BY salary DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rank FROM vw_tbl_emp_window_f;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT name, salary, dep_name, ROW_NUMBER() OVER(PARTITION BY dep_name ORDER BY salary DESC) AS rank FROM vw_tbl_emp_window_f;\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c1f62c82bb56591ac936d53453301766eb92305bf3878cc3736aaa64eb60126"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
